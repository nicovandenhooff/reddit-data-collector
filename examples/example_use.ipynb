{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209365df",
   "metadata": {},
   "source": [
    "*Ignore this first part, it's just housekeeping to get the rest of the example below to load properly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c33673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nicovandenhooff/Dropbox/GITHUB/reddit-data-collector/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ee31d3",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../tests/credentials.json\") as f:\n",
    "    login = json.load(f)\n",
    "\n",
    "    client_id = login[\"client_id\"]\n",
    "    client_secret = login[\"client_secret\"]\n",
    "    user_agent = login[\"user_agent\"]\n",
    "    username = login[\"username\"]\n",
    "    password = login[\"password\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c2b6b3",
   "metadata": {},
   "source": [
    "# Example Use of the Reddit Data Collector Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ebf7e1",
   "metadata": {},
   "source": [
    "### Step 1: Create `DataCollector` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92471743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reddit_data_collector as rdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6419e44-e5e7-469a-8a3d-bc0fbcf8ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collector = rdc.DataCollector(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent,\n",
    "    username=username,\n",
    "    password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69227c75",
   "metadata": {},
   "source": [
    "### Step 2: Obtain some post and comment data from Reddit\n",
    "\n",
    "In this section we:\n",
    "- Obtain 10 \"hot\" posts, their comments, and the comment replies from the subreddits **r/pics** and **r/funny**.\n",
    "- We set the `replace_more_limit` to `0`, which means that any instances of comments that are returned as `MoreComment` are **removed**.\n",
    "    - See the [PRAW Documentation](https://praw.readthedocs.io/en/stable/tutorials/comments.html) for full details on `MoreComment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b49388f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting hot r/pics posts: 100%|██████████████| 10/10 [00:00<00:00, 23.92it/s]\n",
      "Collecting hot r/funny posts: 100%|█████████████| 10/10 [00:00<00:00, 23.22it/s]\n",
      "Collecting comments for 10 r/pics posts: 100%|██| 10/10 [00:10<00:00,  1.03s/it]\n",
      "Collecting comments for 10 r/funny posts: 100%|█| 10/10 [00:14<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "posts, comments = data_collector.get_data(\n",
    "    subreddits=[\"pics\", \"funny\"],\n",
    "    post_filter=\"hot\",\n",
    "    post_limit=10,\n",
    "    comment_data=True,\n",
    "    replies_data=True,\n",
    "    replace_more_limit=0,\n",
    "    dataframe=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c1f5578-82ae-4b58-a415-5f6181876d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 15 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   subreddit_name       20 non-null     object \n",
      " 1   post_created_utc     20 non-null     float64\n",
      " 2   id                   20 non-null     object \n",
      " 3   is_original_content  20 non-null     bool   \n",
      " 4   is_self              20 non-null     bool   \n",
      " 5   link_flair_text      2 non-null      object \n",
      " 6   locked               20 non-null     bool   \n",
      " 7   num_comments         20 non-null     int64  \n",
      " 8   over_18              20 non-null     bool   \n",
      " 9   score                20 non-null     int64  \n",
      " 10  spoiler              20 non-null     bool   \n",
      " 11  stickied             20 non-null     bool   \n",
      " 12  title                20 non-null     object \n",
      " 13  upvote_ratio         20 non-null     float64\n",
      " 14  url                  20 non-null     object \n",
      "dtypes: bool(6), float64(2), int64(2), object(5)\n",
      "memory usage: 1.6+ KB\n"
     ]
    }
   ],
   "source": [
    "posts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bfdd82d-0e6d-450c-af7f-fad9848a09ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3918 entries, 0 to 3917\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   subreddit_name       3918 non-null   object \n",
      " 1   id                   3918 non-null   object \n",
      " 2   post_id              3918 non-null   object \n",
      " 3   parent_id            3918 non-null   object \n",
      " 4   top_level_comment    3918 non-null   bool   \n",
      " 5   body                 3918 non-null   object \n",
      " 6   comment_created_utc  3918 non-null   float64\n",
      " 7   is_submitter         3918 non-null   bool   \n",
      " 8   score                3918 non-null   int64  \n",
      " 9   stickied             3918 non-null   bool   \n",
      "dtypes: bool(3), float64(1), int64(1), object(5)\n",
      "memory usage: 225.9+ KB\n"
     ]
    }
   ],
   "source": [
    "comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af356a6c",
   "metadata": {},
   "source": [
    "### Step 3: Save post and comment data as `.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4701c3d4",
   "metadata": {},
   "source": [
    "In this section we simply save our data collected as `.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "101dd588",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.to_csv(\"../examples/example_posts.csv\", index=False)\n",
    "comments.to_csv(\"../examples/example_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2154f9c",
   "metadata": {},
   "source": [
    "### Step 4: Collect some more post and comment data from Reddit\n",
    "- Now we collect some additional post and comment data from the same subreddits\n",
    "- The post data we collect now is filtered by the \"top\" daily posts\n",
    "- Unlike step 2, we only obtain top level comment data and not individual replies to each comment, which helps speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acdf8190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting top r/pics posts: 74it [00:00, 74.14it/s]\n",
      "Collecting top r/funny posts: 100it [00:01, 62.50it/s]\n",
      "Collecting comments for 74 r/pics posts: 100%|██| 74/74 [00:25<00:00,  2.96it/s]\n",
      "Collecting comments for 100 r/funny posts: 100%|█| 100/100 [01:21<00:00,  1.23it\n"
     ]
    }
   ],
   "source": [
    "more_posts, more_comments = data_collector.get_data(\n",
    "    subreddits=[\"pics\", \"funny\"],\n",
    "    post_filter=\"top\",\n",
    "    top_post_filter=\"day\",\n",
    "    comment_data=True,\n",
    "    replies_data=False,\n",
    "    replace_more_limit=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f393b82c-90c1-4d4e-85cb-e062259383b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38fadd28-3ccd-4d96-b117-4ca8ed3d2c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3978, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_comments.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5970993",
   "metadata": {},
   "source": [
    "### Step 5: Update existing `.csv` files with additional data collected\n",
    "- Now we can add our new post and comment data to the existing `.csv` files\n",
    "- There is a convenience function called `update_data` in the `reddit_data_collector` package that allows us to do this easily\n",
    "- This function is mindful to not save duplicate data\n",
    "- This function includes an argument `save` that if set to `True` will overwrite the old `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a426a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where we saved post and comment data in step 4\n",
    "posts_csv_path = \"../examples/example_posts.csv\"\n",
    "comments_csv_path = \"../examples/example_comments.csv\"\n",
    "\n",
    "updated_posts = rdc.update_data(\n",
    "    posts_csv_path,\n",
    "    more_posts,\n",
    "    save=True\n",
    ")\n",
    "\n",
    "updated_comments = rdc.update_data(\n",
    "    comments_csv_path,\n",
    "    more_comments,\n",
    "    save=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45caf672-9e61-48fd-97b7-0f06cdaf013d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts collected...\n",
      "First collection: 20\n",
      "Second collection: 174\n",
      "After merging: 176\n"
     ]
    }
   ],
   "source": [
    "print(\"Posts collected...\")\n",
    "print(f\"First collection: {posts.shape[0]}\")\n",
    "print(f\"Second collection: {more_posts.shape[0]}\")\n",
    "print(f\"After merging: {updated_posts.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c09c4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments collected...\n",
      "First collection: 3918\n",
      "Second collection: 3978\n",
      "After merging: 6066\n"
     ]
    }
   ],
   "source": [
    "print(\"Comments collected...\")\n",
    "print(f\"First collection: {comments.shape[0]}\")\n",
    "print(f\"Second collection: {more_comments.shape[0]}\")\n",
    "print(f\"After merging: {updated_comments.shape[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:reddit-data-collector]",
   "language": "python",
   "name": "conda-env-reddit-data-collector-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
