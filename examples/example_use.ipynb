{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209365df",
   "metadata": {},
   "source": [
    "*Ignore this first part, it's just housekeeping to get the rest of the example below to load properly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c33673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nicovandenhooff/Dropbox/GITHUB/reddit-data-collector/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ee31d3",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../tests/credentials.json\") as f:\n",
    "    login = json.load(f)\n",
    "    \n",
    "    client_id = login[\"client_id\"]\n",
    "    client_secret = login[\"client_secret\"]\n",
    "    user_agent = login[\"user_agent\"]\n",
    "    username = login[\"username\"]\n",
    "    password = login[\"password\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c2b6b3",
   "metadata": {},
   "source": [
    "# Example Use of the Reddit Data Collector Package\n",
    "\n",
    "Steps:\n",
    "1. Create `DataCollector` object\n",
    "2. Obtain some post and comment data from Reddit\n",
    "3. Convert post and comment data to `pandas` `DataFrame`\n",
    "4. Save post and comment data as `.csv` files via `pandas`\n",
    "5. Collect some more post and comment data from Reddit\n",
    "6. Add new post and comment data to existing `.csv` files with a convenience function in the Reddit Data Collector package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ebf7e1",
   "metadata": {},
   "source": [
    "### Step 1: Create `DataCollector` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92471743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reddit_data_collector as rdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6419e44-e5e7-469a-8a3d-bc0fbcf8ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collector = rdc.DataCollector(\n",
    "    client_id=client_id, \n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent,\n",
    "    username=username,\n",
    "    password=password\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69227c75",
   "metadata": {},
   "source": [
    "### Step 2: Obtain some post and comment data from Reddit\n",
    "\n",
    "In this section we:\n",
    "- Obtain posts from the subreddits **r/pics** and **r/funny**\n",
    "- The category that the posts are in is `hot`\n",
    "- We limit our number of posts returned to `10` (the max is 1,000 per the Reddit API)\n",
    "- We also set `comment_data` and `replies_data` to `True`, which means we also obtain all the comments on the 10 posts above, and the replies to each comment (subject to the point below)\n",
    "- We set the `replace_more_limit` to `0`, which means that any instances of comments that are returned as `MoreComment`s by `PRAW` (the Reddit API python wrapper package that this package is built on) are **removed**\n",
    "    - `MoreComment`s represent when a thread in Reddit says “load more comments”, or “continue this thread”.\n",
    "    - Setting this as any integer greater than `0` means we would replace `MoreComments` instances with PRAW `Comment` instances (i.e. more valid comments from the Reddit post) until either this value is met or we replace all of them.\n",
    "    - To ensure all `MoreComment`s are replaced with valid comment values set the value of `replace_more_limit` to `None`.\n",
    "    - Note that every `MoreComment` we replace is 1 API call!  So setting this to a high integer value, or especially `None` can significantly slow down the script!  Think about if reply data is even valuable for your purpose, often the replies in comments are trolls talking to one another.\n",
    "    - See the [PRAW Documentation](https://praw.readthedocs.io/en/stable/tutorials/comments.html) for full details on `MoreComment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b49388f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting hot r/pics posts: 100%|██████████████| 10/10 [00:00<00:00, 15.40it/s]\n",
      "Collecting hot r/funny posts: 100%|█████████████| 10/10 [00:00<00:00, 17.92it/s]\n",
      "Collecting comments for 10 r/pics posts: 100%|██| 10/10 [00:21<00:00,  2.15s/it]\n",
      "Collecting comments for 10 r/funny posts: 100%|█| 10/10 [00:11<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "subreddits = [\"pics\", \"funny\"]\n",
    "post_filter = \"hot\"\n",
    "post_limit = 10\n",
    "top_post_filter = None\n",
    "comment_data = True\n",
    "replies_data = True\n",
    "replace_more_limit = 0\n",
    "\n",
    "posts, comments = data_collector.get_data(\n",
    "    subreddits,\n",
    "    post_filter,\n",
    "    post_limit,\n",
    "    top_post_filter,\n",
    "    comment_data,\n",
    "    replies_data,\n",
    "    replace_more_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad20931",
   "metadata": {},
   "source": [
    "### Step 3: Convert post and comment data to `pandas` `DataFrame`\n",
    "\n",
    "In this section we:\n",
    "- Create one `DataFrame` that contains all the subreddit posts for **r/pics** and **r/funny** and one `DataFrame` that contains all the comments for **r/pics** and **r/funny**\n",
    "\n",
    "*Alternative method: Convert to seperate `DataFrame`s for each subreddit*\n",
    "- If desired, the `to_pandas` function takes an argument `seperate` \n",
    "- If we set `seperate=True` then the method returns a `dict` of `DataFrame` objects, one per subreddit \n",
    "- For example:\n",
    "    - In this case if we had ran the code `dfs = rdc.to_pandas(posts, seperate=True)` then we would have received a `dfs` would have been a `dict` that contained 2 `DataFrames`, one that contains the posts for the reddit **r/pics** and one for the subreddit **r/funny**\n",
    "    - The items in each dictionary would be `{\"subreddit_name\": DataFrame of posts}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f338923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = rdc.to_pandas(posts)\n",
    "comments_df = rdc.to_pandas(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b2420",
   "metadata": {},
   "source": [
    "Here we can see that our `DataCollector` collected 10 posts for each subreddit for a total of 20 posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcb7954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 15 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   subreddit_name       20 non-null     object \n",
      " 1   post_created_utc     20 non-null     float64\n",
      " 2   id                   20 non-null     object \n",
      " 3   is_original_content  20 non-null     bool   \n",
      " 4   is_self              20 non-null     bool   \n",
      " 5   link_flair_text      3 non-null      object \n",
      " 6   locked               20 non-null     bool   \n",
      " 7   num_comments         20 non-null     int64  \n",
      " 8   over_18              20 non-null     bool   \n",
      " 9   score                20 non-null     int64  \n",
      " 10  spoiler              20 non-null     bool   \n",
      " 11  stickied             20 non-null     bool   \n",
      " 12  title                20 non-null     object \n",
      " 13  upvote_ratio         20 non-null     float64\n",
      " 14  url                  20 non-null     object \n",
      "dtypes: bool(6), float64(2), int64(2), object(5)\n",
      "memory usage: 1.6+ KB\n"
     ]
    }
   ],
   "source": [
    "posts_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ae48ba",
   "metadata": {},
   "source": [
    "Here we can see that our `DataCollector` collected 4,482 comments from both subreddits in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e9d77c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4580 entries, 0 to 4579\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   subreddit_name       4580 non-null   object \n",
      " 1   id                   4580 non-null   object \n",
      " 2   post_id              4580 non-null   object \n",
      " 3   parent_id            4580 non-null   object \n",
      " 4   top_level_comment    4580 non-null   bool   \n",
      " 5   body                 4580 non-null   object \n",
      " 6   comment_created_utc  4580 non-null   float64\n",
      " 7   is_submitter         4580 non-null   bool   \n",
      " 8   score                4580 non-null   int64  \n",
      " 9   stickied             4580 non-null   bool   \n",
      "dtypes: bool(3), float64(1), int64(1), object(5)\n",
      "memory usage: 264.0+ KB\n"
     ]
    }
   ],
   "source": [
    "comments_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af356a6c",
   "metadata": {},
   "source": [
    "### Step 4: Save post and comment data as `.csv` files via `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4701c3d4",
   "metadata": {},
   "source": [
    "In this section we simply save our data collected as `.csv` files with `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "101dd588",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df.to_csv(\"../examples/example_posts.csv\", index=False)\n",
    "comments_df.to_csv(\"../examples/example_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2154f9c",
   "metadata": {},
   "source": [
    "### Step 5: Collect some more post and comment data from Reddit\n",
    "- Here we collect some more post and comment data from the same subreddits\n",
    "- The post data we collect now is filtered by the `top` posts for the `day`\n",
    "- We collect a total of 100 posts\n",
    "- This value is determined by the Reddit API with a max of 100, and we cannot set it manually, even within `praw`.\n",
    "- Unlike step 2, we only obtain top level comment data and not individual replies to each comment.\n",
    "- Trying to collect comment and reply data for top posts on popular subreddits can take arbitrarily long depending on the depth of comments and replies on a post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acdf8190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting top r/pics posts: 100it [00:01, 63.32it/s]\n",
      "Collecting top r/funny posts: 100it [00:01, 65.34it/s]\n",
      "Collecting comments for 100 r/pics posts: 100%|█| 100/100 [00:58<00:00,  1.72it/\n",
      "Collecting comments for 100 r/funny posts: 100%|█| 100/100 [00:39<00:00,  2.52it\n"
     ]
    }
   ],
   "source": [
    "subreddits = [\"pics\", \"funny\"]\n",
    "post_filter = \"top\"\n",
    "top_post_filter = \"day\"\n",
    "comment_data = True\n",
    "replies_data = False\n",
    "replace_more_limit = 0\n",
    "\n",
    "more_posts, more_comments = data_collector.get_data(\n",
    "    subreddits,\n",
    "    post_filter,\n",
    "    post_limit,\n",
    "    top_post_filter,\n",
    "    comment_data,\n",
    "    replies_data,\n",
    "    replace_more_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c1a28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_posts_df = rdc.to_pandas(more_posts)\n",
    "more_comments_df = rdc.to_pandas(more_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c040113b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 15)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "100af7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3621, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_comments_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5970993",
   "metadata": {},
   "source": [
    "### Step 6: Add new post and comment data to existing `.csv` files with a convenience function in the Reddit Data Collector package\n",
    "- Now we can add our new post and comment data to the existing `.csv` files\n",
    "- There is a convenience function called `update_data` in the `reddit_data_collector` package that allows us to do this easily\n",
    "- This function is mindful to not save duplicate data\n",
    "- This function includes an argument `save` that if set to `True` will overwrite the old `.csv` file\n",
    "- It also returns the new updated data as a `pandas` `DataFrame` in case the user desires to manipulate it in Python right away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a426a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where we saved post and comment data in step 4\n",
    "existing_posts_csv_path = \"../examples/example_posts.csv\"\n",
    "existing_comments_csv_path = \"../examples/example_comments.csv\"\n",
    "\n",
    "new_posts_df = rdc.update_data(\n",
    "    existing_posts_csv_path,\n",
    "    more_posts_df,\n",
    "    save=True\n",
    ")\n",
    "\n",
    "new_comments_df = rdc.update_data(\n",
    "    existing_comments_csv_path,\n",
    "    more_comments_df,\n",
    "    save=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd725a98",
   "metadata": {},
   "source": [
    "We see that 3 new posts we added to our data, that means we had a few duplicates in the `hot` posts we collected and the `top` posts which makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a56762ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c09c4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6918, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_comments_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb31df9",
   "metadata": {},
   "source": [
    "## Concluding remarks\n",
    "\n",
    "In a perfect world, we could just collect all the data that we needed from Reddit in one go, and not have to iterate like the above.  For example it would be ideal if we could just collect data as easy as \"collect all hot posts from subreddit X from 2015 to 2020.\"  Unfortunately, the Reddit API does not allow for this.\n",
    "\n",
    "Therefore, I designed this package, with the idea that this script could be used to collect samples of post and comment data from Reddit at multiple time periods (e.g. daily at 5pm), and then these samples could be combined into one data set seamlessly.\n",
    "\n",
    "An example automated workflow to generate a data set would be:\n",
    "\n",
    "Write a python script that runs every day at 5pm and:\n",
    "\n",
    "1. Collects post and comment data from Reddit with the Reddit Data Collector\n",
    "2. Adds post and comment data to a `.csv` file, updating it each time\n",
    "\n",
    "At the end of the month we have 30 days worth of Reddit data that we can now further analyze!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:reddit-data-collector]",
   "language": "python",
   "name": "conda-env-reddit-data-collector-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
